# Human Activity Recogniztion Using Accelerometer data and ESP-DL 

Edge computing is a distributed computing paradigm that brings computation and data storage closer to the device's location. Edge Artificial Intelligence (edge-AI) is an exciting development within Edge computing because it allows traditional technologies to run more efficiently, with higher performance and less power. Trained neural networks are used to make inferences on small devices. The potential [applications areas](https://www.xenonstack.com/blog/edge-ai-use-case) of edge AI includes manufacturing, Healthcare, the Retail industry, Surveillance, smart home, and Finance banking.

[Espressif System](https://www.espressif.com/) provides a framework [ESP-DL](https://github.com/espressif/esp-dl) that can be used to deploy your high-performance deep learning models on various Espressif chip-sets [ESP32](https://www.espressif.com/en/products/socs/esp32), [ESP32-S2](https://www.espressif.com/en/products/socs/esp32-s2), [ESP32-S3](https://www.espressif.com/en/products/socs/esp32-s3) and [ESP32-C3](https://www.espressif.com/en/products/socs/esp32-c3). 

*In this article, we will understand how to read sensor data and using [ESP-DL](https://github.com/espressif/esp-dl) to [deploy](https://github.com/espressif/esp-dl/tree/master/tutorial/quantization_tool_example) a deep-learning model on [ESP32-S3](https://www.espressif.com/en/products/socs/esp32-s3).*

---
## Prerequisite for using ESP-DL
* Before getting a deep dive into ESP-DL, we assume that readers have;  

    1. Knowledge about building and training neural networks.<sup>  [(deep learning basics with python)](https://www.youtube.com/watch?v=WvoLTXIjBYU) 
    2. Configure the ESP-IDF [release 4.4](https://github.com/espressif/esp-idf/tree/release/v4.4)  environment. <sup>[setting-up ESP-IDF environment](https://www.youtube.com/watch?v=byVPAfodTyY)/[toolchain for ESP-IDF](https://blog.espressif.com/esp-idf-development-tools-guide-part-i-89af441585b) 
    3. Working knowledge of basic C and C++ language.<sup>[C - language tutorial](https://www.youtube.com/watch?v=KJgsSFOSQv0&t=12665s)
    4. Converting model into ESP-DL formate. <sup>[Hand Gesture Recognition on ESP32-S3 with ESP-Deep Learning](https://medium.com/the-esp-journal/hand-gesture-recognition-on-esp32-s3-with-esp-deep-learning-176d7e13fd37)

---
## 1. Model Deployment
A simple Convolution neural network is designed using [accelerometer data](https://www.cis.fordham.edu/wisdm/dataset.php) to recognize the human activity. 

<sub> In this blog we will not focus on the [development](https://www.youtube.com/watch?v=lUI6VMj43PE) and conversion of neural network to [ESP-Dl format](https://medium.com/the-esp-journal/hand-gesture-recognition-on-esp32-s3-with-esp-deep-learning-176d7e13fd37).

### 1.1. ESP-IDF Project Hierarchy

- The first step is to create a new project in VS-Code based on ESP-IDF standards. For more details about how to create a VScode project for ESP32 please [click here](https://www.youtube.com/watch?v=Lc6ausiKvQM) or [here](https://docs.espressif.com/projects/esp-idf/en/latest/esp32/get-started/linux-macos-setup.html)
- Place the files .cpp and .hpp generated by converting the model into ESP-DL format in your current working directory.  
- Add all the dependent components to the components folder of your working directory. 
- sdk.config files are default files from [esp-who example](https://github.com/espressif/esp-who/tree/master/examples/human_face_recognition/terminal). These files are also provided in linked [GITHUB](https://github.com/alibukharai/Blogs/tree/main/ESP-DL)

The Project directory should look like this;

```cpp 
├── CMakeLists.txt
├── components
│   ├── bus
│   ├── mpu6050
│   └── esp-dl
├── dependencies.lock
├── main
│   ├── app_main.cpp
│   └── CMakeLists.txt
├── model
│   ├── Activity_coefficient.cpp
│   ├── Activity_coefficient.hpp
│   └── model_define.hpp
├── partitions.csv
├── sdkconfig
├── sdkconfig.defaults
├── sdkconfig.defaults.esp32
├── sdkconfig.defaults.esp32s2
└── sdkconfig.defaults.esp32s3

```
### 1.2. Model define

We will define our model in the 'model_define.hpp' file. Follow the below steps for a details explanation of defining the model.   

#### 3.2.1. Import libraries
Firstly import all the relevant libraries. Based on our [model design](#13-building-a-model) or another way to know which particular libraries need to use an open source tool [Netron](https://netron.app/) and open your optimized ONNX model generated at the end of [previous section 2.2](#22-optimization-and-quantization).
Please [check here](https://github.com/espressif/esp-dl/tree/master/include/layer) for all the currently supported libraries by [ESP-DL](https://github.com/espressif/esp-dl). 

```cpp
#pragma once
#include "dl_layer_model.hpp"
#include "dl_layer_base.hpp"
#include "dl_layer_max_pool2d.hpp"
#include "dl_layer_conv2d.hpp"
#include "dl_layer_concat.hpp"
#include "Activity_coefficient.hpp"
#include "dl_layer_reshape.hpp"
#include "dl_layer_softmax.hpp"
#include <stdint.h>

using namespace dl;
using namespace layer;
using namespace Activity_coefficient;

```
#### 3.2.2. Declare layers
The next is to declare each layer. 
- Input is not considered a layer so not defined here.
- Except for the output layer, all the layers are declared as private layers.
- Remember to place each layer in order as defined in [previous section 1.3.](#13-building-a-model) while building the model. 

```cpp
class ACTIVITY : public Model<int16_t> 
{
private:
    Conv2D<int16_t> l1;
    Conv2D<int16_t> l2;
    Reshape<int16_t> l3;
    Conv2D<int16_t> l4;
    Conv2D<int16_t> l5;
    

public:
    Softmax<int16_t> l6;

```

#### 3.2.3. Initialize layers 

After declaring the layers, we need to initialize each layer with its weight, biases activation functions and shape. let us check each layer in detail. 

Before getting into details, let us look into how our model looks like when opening in Netron that is somehow imported to get some parameters for initializing. 

<p align="center">
    <img src="./_static/6.png#center">


- The first layer is reshaped layer (note that the input is not considered as a layer) and gives an output shape of (96 , 96, 1) for this layer. These parameters must be the same as you used during model training [see section 1.3.](#13-building-a-model) Another way to know the parameter and layer is to use an open source tool [Netron](https://netron.app/) and open your optimized ONNX model generated at the end of [previous section 2.2.](#22-optimization-and-quantization). 

- For the convolution 2D layer we can get the name of this layer for the filter, bias and activation function from the .hpp file generated at the end of the [previous section 2.2.](#22-optimization-and-quantization), However for the exponents, we need to check the output generated in [section 2.2.5.](#225-calibration)  

- For the max-pooling layer, we can use the same parameters as we use during building our model [see section 1.3.](#13-building-a-model) or another way to know the parameter and layer is to use an open-source tool [Netron](https://netron.app/) and open your optimized ONNX model generated at the end of the [previous section 2.2.](#22-optimization-and-quantization).

- For the dense layer or fully connected layer, conv2D block is used and we can get the name of this layer for the filter, bias and activation function from the .hpp file generated at the end of [previous section 2.2.](#22-optimization-and-quantization), However for the exponents, we need to check the output generated in [section 2.2.5.](#225-calibration)

- The output layer is a softmax layer weight and the name can be taken from the output generated in [section 2.2.5.](#225-calibration)
 

```cpp
    ACTIVITY () : 
                         l1(Conv2D<int16_t>(-13, get_statefulpartitionedcall_sequential_1_conv2d_2_biasadd_filter(), get_statefulpartitionedcall_sequential_1_conv2d_2_biasadd_bias(), get_statefulpartitionedcall_sequential_1_conv2d_2_biasadd_activation(), PADDING_VALID, {}, 1,1, "l1")),
                         l2(Conv2D<int16_t>(-13, get_statefulpartitionedcall_sequential_1_conv2d_3_biasadd_filter(), get_statefulpartitionedcall_sequential_1_conv2d_3_biasadd_bias(), get_statefulpartitionedcall_sequential_1_conv2d_3_biasadd_activation(), PADDING_VALID, {}, 1,1, "l2")),                       
                         l3(Reshape<int16_t>({1,1,2496},"l2_reshape")), 
                         l4(Conv2D<int16_t>(-11, get_fused_gemm_0_filter(), get_fused_gemm_0_bias(), get_fused_gemm_0_activation(), PADDING_VALID, {}, 1, 1, "l3")),
                         l5(Conv2D<int16_t>(-9, get_fused_gemm_1_filter(), get_fused_gemm_1_bias(), NULL, PADDING_VALID,{}, 1,1, "l4")),
                         l6(Softmax<int16_t>(-14,"l5")){}

```
#### 3.2.4. Build layers

The next step is to build each layer. For more information about building layers please [click here](https://github.com/espressif/esp-dl/tree/master/include/layer) on each layer building function.  

```cpp
void build(Tensor<int16_t> &input)
    {
        this->l1.build(input);
        this->l2.build(this->l1.get_output());
        this->l3.build(this->l2.get_output());
        this->l4.build(this->l3.get_output());
        this->l5.build(this->l4.get_output());
        this->l6.build(this->l5.get_output());
        
    }

```
#### 3.2.5. Call layers

In the end, we need to connect these layers and call them one by one by using a call function. For more information about calling layers please [click here](https://github.com/espressif/esp-dl/tree/master/include/layer) on each layer calling function.

```cpp
void call(Tensor<int16_t> &input)
    {
        this->l1.call(input);
        input.free_element();

        this->l2.call(this->l1.get_output());
        this->l1.get_output().free_element();

        this->l3.call(this->l2.get_output());
        this->l2.get_output().free_element();

        this->l4.call(this->l3.get_output());
        this->l3.get_output().free_element();

        this->l5.call(this->l4.get_output());
        this->l4.get_output().free_element();

        this->l6.call(this->l5.get_output());
        this->l5.get_output().free_element();

    }
};

```
### 3.3. Model Run
After building our Model need to run and give input to our model. 'app_main.cpp' file is used to generate the input and run our model on [ESP32-S3](https://www.espressif.com/en/products/socs/esp32-s3). 

#### 3.3.1. import libraries
```c
#include <stdio.h>
#include <stdlib.h>
#include "esp_system.h"
#include "freertos/FreeRTOS.h"
#include "freertos/task.h"
#include "dl_tool.hpp"
#include "model_define.hpp"
#include "i2c_bus.h"
#include "mpu6050.h"
#include "driver/i2c.h"
#include "esp_log.h"

```

#### 3.3.2. Declare Input 
we trained our model by giving an input of size (96, 96, 1) [see section 1.3.](#13-building-a-model) However, the input_exponent can get its exponent value from the output generated in [section 2.2.5.](#225-calibration) Another thing is to write the pixels of the input/test picture here. 

```cpp
int input_height = 96;
int input_width = 96;
int input_channel = 1;
int input_exponent = -7;

__attribute__((aligned(16))) int16_t example_element[] = {

    //add your input/test image pixels 
};

```

#### 3.3.3. Set Input Shape

Each pixel of the input is adjusted based on the input_exponent declared [above](#332-declare-input).   

```cpp
Tensor<int16_t> input;

                input.set_element((int16_t *) model_input).set_exponent(input_exponent).set_shape({input_height,input_width,input_channel}).set_auto_free(false);

```

#### 3.3.4. Call Model 
Call the model by calling the method forward and passing input to it. Latency is used to calculate the time taken by ESP32-S3 to run the neural network. 

```cpp
ACTIVITY model;
                dl::tool::Latency latency;
                latency.start();
                model.forward(input);
                latency.end();
                latency.print("\nActivity model", "forward");

```

#### 3.3.5. Monitor Output 

The output is taken out from the public layer i.e l11. and you can print the result in the terminal. 

```cpp
float *score = model.l6.get_output().get_element_ptr();
                float max_score = score[0];
                int max_index = 0;
                for (size_t i = 0; i < 6; i++)
                {
                    printf("%f, ", score[i]*100);
                    if (score[i] > max_score)
                    {
                        max_score = score[i];
                        max_index = i;
                    }
                }
                printf("\n");
                switch (max_index)
                {
                    case 0:
                    printf("0: Downstairs");
                    break;
                    case 1:
                    printf("1: Jogging");
                    break;
                    case 2:
                    printf("2: Sitting");
                    break;
                    case 3:
                    printf("3: Standing");
                    break;
                    case 4:
                    printf("4: Upstairs");
                    break;
                    case 5:
                    printf("5: Walking");
                    break;
                    default:
                    printf("No result");
                }
                printf("\n");
}
}

```
## 4. Future Directions
In future, we will design a model for [ESP32-S3 EYE](https://www.espressif.com/en/products/devkits/esp-eye/resourceswww.espressif.com2) devkit which could capture images real time and do hand gesture recognition.